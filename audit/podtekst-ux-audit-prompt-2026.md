# PodTeksT — Comprehensive UX/UI Audit (2026 Standards)

## Context

You are auditing the UX/UI of PodTeksT, a Polish chat analysis application. Users upload exported conversations (Messenger, WhatsApp, Instagram, Telegram, Discord), the app parses them in the browser, calculates 80+ metrics, runs 4 AI passes via Gemini API, and presents results across 10+ sections with downloadable cards, badges, roasts, simulations, and more.

The app has a dark theme with purple/violet accent colors, HDR-optimized gradients, and heavy visual design. The target audience is Polish-speaking young adults (18-30) who want to analyze their chat dynamics — part serious psychology, part entertainment with uncensored humor.

This is a REAL product heading to market in 2026. The UX must reflect current standards, not 2023 patterns.

## Your Task

Go through EVERY page, component, layout, and user flow in the codebase. Analyze the full experience from landing page through analysis completion. Read the actual code — CSS, Tailwind classes, component structure, state management, routing, loading states, error handling.

## Audit Scope

### A. First-Time User Flow (Critical Path)

Trace the EXACT journey of a new user who knows nothing about the app:

1. **Landing page → Upload** — How many clicks/steps from landing to starting analysis? Is the CTA obvious? Is "upload your chat export" self-explanatory or does the user need to know what that means?
2. **Export instructions** — Does the app explain HOW to export chats from each platform? With screenshots? Step by step? This is the #1 drop-off point — if someone doesn't know how to export from Messenger, they leave.
3. **File upload → Parsing** — What feedback does the user get during parsing? Progress bar? Estimated time? What happens if parsing fails? What's the error message?
4. **Parsing → Analysis** — Transition from local parsing to AI analysis. Is it clear what's happening? How long does it take? Is there a loading state that communicates progress?
5. **Analysis → Results** — How are results presented? Is the user overwhelmed or guided? Is there a logical reading order?

Map every possible drop-off point and dead end in this flow.

### B. AI Transparency & Explainability (CRITICAL FOR 2026)

This is the single most important UX trend of 2026. The EU AI Act (applicable 2026) requires users to be informed when they interact with AI systems. Beyond compliance, explainable AI (XAI) is now a core UX expectation, not a nice-to-have.

PodTeksT is an AI-heavy product. Every AI-generated result MUST be transparent. Audit:

1. **AI vs Algorithmic distinction** — Can the user tell which results are calculated locally in the browser (deterministic, reproducible) vs generated by Gemini AI (probabilistic, may vary)? This MUST be visually distinct. Check if there are:
   - AI labels/badges on AI-generated content (see IBM Carbon's AI label pattern)
   - Different visual treatment for AI sections vs computed metrics
   - Clear indicators like "Generated by AI" vs "Calculated from your data"

2. **"Why this result?" explainability** — For every AI-generated insight, roast, score, or profile:
   - Is there a way to see WHY the AI said what it said?
   - Are there expandable rationale chips or "Why this?" tooltips?
   - Progressive disclosure: summary visible, reasoning expandable on demand
   - Source attribution: does the AI reference specific messages that led to conclusions?

3. **Confidence indicators** — Does the UI communicate reliability?
   - Metrics based on 10,000 messages should look different from metrics based on 50
   - AI-generated content should indicate certainty level
   - Experimental metrics should be marked as such
   - Pattern: confidence bars, reliability badges, sample size indicators

4. **Reproducibility expectations** — Does the user understand that:
   - Running the same analysis twice may produce different AI results?
   - Local metrics will be identical but AI insights may vary?
   - Is this communicated anywhere?

5. **AI limitations disclosure** — At the RIGHT moment (not buried in ToS):
   - "Results are for entertainment, not clinical diagnosis"
   - "AI analysis is based on ~2% of your conversation"
   - Limitations of sentiment analysis on Polish text
   - Visible at point of consumption, not just on landing page

### C. Privacy-First Design Patterns (2026 Standard)

Privacy-first UX is no longer about a privacy policy link in the footer. In 2026, transparent data handling with clear consent mechanisms is a baseline expectation.

1. **Upload moment** — When user is about to upload their chat, do they see:
   - Exactly what will happen with their data?
   - What stays local vs what goes to Gemini API?
   - A clear consent step, not just "by using this you agree"?
   - The "~2% to AI" message at the moment it matters, not just on landing page?

2. **Data flow visualization** — Is there a clear, visual explanation of:
   - File → parsed in browser (local) → metrics calculated (local) → samples to AI → results stored in IndexedDB (local)?
   - Users in 2026 expect to SEE where their data goes, not read about it

3. **Sharing consent** — When sharing analysis:
   - Is it clear what the recipient will see?
   - Can the sharer preview exactly what will be shared?
   - Is the other person's consent addressed?

4. **Data deletion** — Can users delete their analyses? Is it obvious how? Is it clear that deletion is real (IndexedDB, not just UI)?

### D. Information Architecture

1. **Navigation** — Bottom nav bar has: EMOCJE, ACR, KŁÓTNIA, AI DEEP DIVE, METRYKI, EKSPORT. Evaluate:
   - Does a first-time user know what "ACR" means?
   - Is there a recommended reading order or progressive disclosure?
   - Should sections unlock/reveal as user explores, rather than showing everything at once?

2. **Content density** — Each section has charts, numbers, text, cards. For 2026:
   - Bento grid layouts: are dense data sections organized in modular blocks of varying sizes for scanability?
   - Is there a clear visual hierarchy within each section?
   - Can a user scan and understand the key insight in 5 seconds per section?

3. **Labels and terminology** — Go through EVERY label, tooltip, section title, button text:
   - Jargon without explanation (LSM, Gottman, Pursuit-Withdrawal) — does each have an accessible explanation?
   - Inconsistency (same concept, different names)
   - English leftovers in a Polish app
   - Psychological terms should have plain-language tooltips

### E. Visual Design & Readability

1. **Contrast ratios** — Dark theme with purple accents. Check EVERY text element against WCAG AA (4.5:1 normal, 3:1 large text). Especially:
   - Subtitle/secondary text on dark backgrounds
   - Disabled states
   - Chart labels and axis text
   - Tooltip text
   - Card descriptions
   - THIS IS LIKELY THE BIGGEST PROBLEM — purple-on-dark is notoriously low contrast

2. **Non-HDR displays** — The design is HDR-optimized. On standard sRGB (most users):
   - Are there elements that become invisible or unreadable?
   - Do gradients flatten or band?
   - Are glowing effects lost, leaving plain text on near-black?
   - TEST THIS: check if any element relies on HDR luminance for readability

3. **Glassmorphism check** — If using translucent/glass-like elements (trending in 2026 dark mode):
   - Does text remain readable over blurred backgrounds?
   - Does it degrade gracefully on low-end devices?
   - Apple's rule: glass effects never compromise text readability

4. **Typography hierarchy** — Clear heading/body/caption distinction at a glance?

5. **Color semantics** — Red = bad, green = good consistently? Colorblind safe?

### F. Responsive Design & Mobile

This app will be shared via links on social media. Most traffic will be mobile.

1. **Mobile layout (375px):**
   - Bottom nav on small screens — does it work?
   - Charts/graphs scale properly?
   - Cards readable?
   - Tap targets minimum 44x44px?
   - Modals/overlays work?
   - Heavy animations — do they kill performance on mid-range phones?

2. **Tablet (768-1024px)** — Layout gaps?

3. **Wide screens (1440px+)** — Stretching or proper max-width constraints?

4. **Orientation** — Landscape breaks anything?

### G. Loading, Error & Empty States

Where most apps fall apart. Check EVERY async operation:

1. **Loading states — Progressive feedback (2026 standard):**
   - NOT just a spinner. In 2026, progressive loading with contextual updates is expected
   - "Analyzing emotions..." → "Detecting communication patterns..." → "Generating insights..."
   - Each of the 4 AI passes should show what's happening
   - Estimated time remaining
   - Skeleton loaders for sections, not blank screens
   - The user should feel like they're watching the analysis build, not waiting for a black box

2. **Error states:**
   - Invalid file format — helpful message with supported formats
   - Corrupted export — guidance on re-exporting
   - File too large/small — specific limits stated
   - Gemini API failure (rate limit, timeout, server error) — graceful degradation, can local metrics still show?
   - Network loss during analysis — can it resume?
   - Browser out of memory — warning before it happens for large files
   - IndexedDB full — actionable message
   - EVERY error message: in Polish, helpful, tells user what to do next, maintains the app's tone

3. **Empty states:**
   - Section with no data (no reactions in chat)
   - Metric incalculable (insufficient data)
   - New account, no analyses
   - Each should guide the user, not just say "Brak danych"

4. **Edge case UX:**
   - Close tab during analysis — resume possible?
   - Navigate away and return?
   - Upload second chat while first analyzes?
   - Chat with 10 messages? 100? 500,000?
   - What's the minimum viable conversation length and is this communicated?

### H. Interactive Elements & Micro-interactions

1. **Buttons** — Hover, active, disabled, loading states for all?
2. **Cards (22+ downloadable)** — Generation time? Preview before download? Format/size info?
3. **Charts** — Interactive tooltips? Labeled axes? Accessible?
4. **Section transitions** — Smooth? State preserved when switching tabs?
5. **Micro-interactions (2026 standard):**
   - Feedback on every user action (not just clicks — state changes, completions)
   - Subtle, functional animations on interaction (not continuous decorative ones)
   - Does the app feel alive and responsive, or static and flat?
   - Are micro-interactions consistent across the app or random?

### I. Performance UX

1. **Perceived performance** — Even if something takes 10 seconds, does it FEEL fast?
   - Progressive loading?
   - Optimistic UI where possible?
   - Content appearing incrementally vs all-at-once after long wait?

2. **Animation audit** — List EVERY animation. Flag:
   - Continuous animations (running without user trigger) — THESE ARE THE #1 PERFORMANCE KILLER
   - Purely decorative with no functional purpose
   - Layout shifts caused by animations
   - Stacking animations
   - `prefers-reduced-motion` NOT respected — this is now a compliance issue, not just nice-to-have
   - Count total active animations on heaviest page

3. **Memory** — Components mounting and never unmounting? Listeners never cleaned up? This was already flagged in the Firefox profiler showing memory leak.

### J. Accessibility (European Accessibility Act — in force 2025)

The European Accessibility Act is now applicable. This isn't optional anymore.

1. **Keyboard navigation** — Full flow completable without mouse?
2. **Screen reader** — Proper aria labels, roles, alt texts on all elements including charts and cards?
3. **Focus management** — Trapped in modals? Logical flow?
4. **`prefers-reduced-motion`** — MUST be respected. All animations should be disabled or reduced.
5. **`prefers-color-scheme`** — Does the app handle light mode requests or force dark? Either is fine but should be intentional.
6. **Font sizing** — Works at 150% browser zoom?
7. **Color-only information** — Any metric that communicates meaning through color alone without text/icon alternative?

### K. Copy & Microcopy

1. **Tone consistency** — Landing page is casual with swearing ("Bez marketingowego pierdolenia"). Does the rest of the app match, or does it switch to corporate/clinical language?
2. **Error messages** — Helpful and human, or generic? Do they match the app's personality?
3. **Tooltips and explanations** — Understandable without a psychology degree? Plain-language summaries of complex metrics?
4. **CTAs** — Action-oriented? "Generuj roast" > "Roast" > "Kliknij tutaj"
5. **AI-generated content framing** — How is AI content introduced? Does it set expectations about tone (especially uncensored roasts)?
6. **Polish language quality** — Any English leftovers? Typos? Awkward phrasing? Inconsistent formal/informal register?

### L. Trust & Conversion

1. **Social proof** — Does the app show that others have used it? Usage counter, testimonials, shared analyses?
2. **Premium vs Free boundary** — Is the paywall clear? Does the user know what they get before paying? No dark patterns? Can they see enough free content to want more?
3. **First analysis delight** — Is the first completed analysis impressive enough to make the user want to share it and come back? What's the "wow moment"?
4. **Share mechanics** — How easy is it to share results? Does it generate shareable images for social media? Is there a viral loop?

## Deliverables

### 1. `UX_AUDIT_REPORT.md`
For each issue:
- **Component/Page** and file location
- **Element reference** (component name, CSS class, or line)
- **Issue** — what's wrong
- **2026 Standard** — what current best practice expects
- **Impact** — user confusion, drop-off, frustration, inaccessibility, legal risk
- **Severity** — CRITICAL (blocks usage or legal risk) / HIGH (significant friction) / MEDIUM (suboptimal) / LOW (polish)
- **Fix** — specific, actionable, with code-level recommendation where possible

### 2. `USER_FLOW_MAP.md`
Complete map of every user path:
- Happy path (upload → analysis → results → share)
- Error paths with recovery options
- Edge cases
- Drop-off points with severity
- AI transparency touchpoints marked

### 3. `ACCESSIBILITY_REPORT.md`
Separate, focused report:
- WCAG AA compliance status per component
- European Accessibility Act gaps
- `prefers-reduced-motion` audit
- Contrast ratio failures (specific elements and values)
- Keyboard navigation map

### 4. `UX_IMPLEMENTATION_PLAN.md`
Prioritized fixes:
- Phase 0: Legal/compliance (accessibility act, AI transparency, RODO consent UX)
- Phase 1: Flow blockers and critical usability issues
- Phase 2: Missing states (loading, error, empty)
- Phase 3: AI explainability features (confidence indicators, "why this?", AI labels)
- Phase 4: Responsive/mobile fixes
- Phase 5: Micro-interactions and polish

## Rules

- Test every component. Don't assume anything works because it looks good in code.
- Think like a 20-year-old who just saw the app on TikTok and has 30 seconds of patience.
- Think like that same 20-year-old on a 3-year-old Android phone with a cracked screen.
- Think like a UODO inspector checking RODO compliance of the data flow.
- Think like an accessibility auditor checking European Accessibility Act compliance.
- If you can't determine something from code alone, flag it as "NEEDS MANUAL TESTING" with exact test instructions.
- Polish language — every user-facing string must be in Polish and make sense.
- Be brutal. Pretty doesn't mean usable. Animations don't mean good UX. Dark theme doesn't mean readable. AI-powered doesn't mean trustworthy.
- 2026 standard: AI transparency is not optional. Privacy UX is not a policy page. Accessibility is law. Progressive loading is baseline. Confidence indicators on AI content are expected.
